#!/usr/bin/env python3
"""
Test anti-overthinking para verificar que el modelo DeepSeek responda concisamente.
"""
import os
import sys
import time
from pathlib import Path

# ConfiguraciÃ³n optimizada anti-overthinking
os.environ["USE_LOCAL_MODELS"] = "true"
os.environ["USE_GPU"] = "true"
os.environ["GPU_LAYERS"] = "25"

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_anti_overthinking():
    """Test para verificar respuestas concisas sin divagaciÃ³n."""
    print("ğŸ¯ TEST ANTI-OVERTHINKING DEEPSEEK")
    print("=" * 50)
    print("Objetivo: Respuestas directas y concisas sin divagaciÃ³n")
    print()
    
    try:
        from src.rag.property_rag_chain import create_rag_chain_from_scraped_data
        
        print("ğŸ¤– Inicializando RAG con configuraciÃ³n anti-overthinking...")
        start_time = time.time()
        rag_chain = create_rag_chain_from_scraped_data()
        init_time = time.time() - start_time
        
        print(f"âœ… Inicializado en {init_time:.2f}s")
        
        # Preguntas diseÃ±adas para provocar overthinking
        test_questions = [
            "Â¿CuÃ¡ntas propiedades hay disponibles?",
            "Â¿Hay departamentos en Independencia?", 
            "Â¿CuÃ¡l es el precio promedio?",
            "MuÃ©strame propiedades de 1 dormitorio",
            "Â¿QuÃ© departamentos cuestan menos de 200000 pesos?"
        ]
        
        print(f"\nğŸ§ª Probando {len(test_questions)} preguntas especÃ­ficas...\n")
        
        results = []
        for i, question in enumerate(test_questions, 1):
            print(f"â“ {i}. {question}")
            
            start_time = time.time()
            answer = rag_chain.ask_question(question)
            response_time = time.time() - start_time
            
            # AnÃ¡lisis de la respuesta
            response_text = answer.answer
            word_count = len(response_text.split())
            line_count = len(response_text.split('\\n'))
            
            # Detectar signos de overthinking
            overthinking_words = ["ademÃ¡s", "tambiÃ©n", "por otro lado", "en conclusiÃ³n", "finalmente", "cabe mencionar"]
            overthinking_count = sum(1 for word in overthinking_words if word.lower() in response_text.lower())
            
            # Mostrar resultado
            print(f"ğŸ’¬ {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
            print(f"ğŸ“Š Palabras: {word_count} | LÃ­neas: {line_count} | Overthinking: {overthinking_count}")
            print(f"â±ï¸ {response_time:.2f}s | ğŸ¯ {answer.confidence:.2f}")
            
            # EvaluaciÃ³n
            if word_count <= 150 and overthinking_count == 0 and response_time <= 15:
                status = "âœ… EXCELENTE"
            elif word_count <= 200 and overthinking_count <= 1 and response_time <= 25:
                status = "ğŸŸ¡ BUENO"
            else:
                status = "âŒ MEJORAR"
            
            print(f"ğŸ“‹ Estado: {status}")
            print("-" * 50)
            
            results.append({
                'question': question,
                'words': word_count,
                'overthinking': overthinking_count,
                'time': response_time,
                'status': status
            })
        
        # Resumen final
        print(f"\nğŸ“ˆ RESUMEN ANTI-OVERTHINKING:")
        print("=" * 50)
        
        excellent = sum(1 for r in results if "EXCELENTE" in r['status'])
        good = sum(1 for r in results if "BUENO" in r['status'])
        poor = sum(1 for r in results if "MEJORAR" in r['status'])
        
        avg_words = sum(r['words'] for r in results) / len(results)
        avg_time = sum(r['time'] for r in results) / len(results)
        total_overthinking = sum(r['overthinking'] for r in results)
        
        print(f"âœ… Excelentes: {excellent}/{len(results)}")
        print(f"ğŸŸ¡ Buenos: {good}/{len(results)}")  
        print(f"âŒ Mejorables: {poor}/{len(results)}")
        print(f"ğŸ“Š Promedio palabras: {avg_words:.1f}")
        print(f"â±ï¸ Promedio tiempo: {avg_time:.2f}s")
        print(f"ğŸ§  Total overthinking: {total_overthinking}")
        
        # EvaluaciÃ³n global
        if excellent >= 3 and total_overthinking <= 2:
            print(f"\nğŸ‰ Â¡ANTI-OVERTHINKING EXITOSO!")
            print("El modelo responde de forma concisa y directa")
        elif excellent + good >= 4:
            print(f"\nâœ… Anti-overthinking funciona bien")
            print("Mayormente respuestas directas")
        else:
            print(f"\nâš ï¸ Necesita mÃ¡s optimizaciÃ³n")
            print("AÃºn hay tendencia a divagar")
        
        return excellent >= 2
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Configurar logging mÃ­nimo
    import logging
    logging.basicConfig(level=logging.WARNING)
    
    success = test_anti_overthinking()
    
    if success:
        print(f"\nğŸ’¡ ConfiguraciÃ³n optimizada funcionando")
        print(f"   - Temperature: 0.0 (determinÃ­stico)")
        print(f"   - Max tokens: 300 (conciso)")
        print(f"   - Stop tokens: agresivos")
        print(f"   - Prompt: directo y estructurado")
    else:
        print(f"\nğŸ”§ Para mejorar aÃºn mÃ¡s:")
        print(f"   - Considera probar otro modelo GGUF")
        print(f"   - Reducir mÃ¡s max_tokens")
        print(f"   - Ajustar stop tokens")
    
    sys.exit(0 if success else 1)