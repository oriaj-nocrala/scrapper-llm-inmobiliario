# üî• VRAM Limits - Sistema GPU Documentado

## üìä Especificaciones del Sistema

**Hardware Detectado:**
- **VRAM Total**: 8GB disponibles
- **VRAM Usable**: ~7.5GB (reservando overhead del sistema)
- **Arquitectura**: Compatible con llama-cpp-python
- **Driver**: Funcionando correctamente con CUDA/OpenCL

## üöÄ Configuraciones Probadas y Resultados

### ‚úÖ **CONFIGURACI√ìN AGRESIVA** (FUNCIONA)
```python
Llama(
    n_ctx=8192,          # 8K contexto ‚úÖ
    n_gpu_layers=30,     # 30 capas GPU ‚úÖ  
    n_batch=1024,        # Batch grande ‚úÖ
    n_threads=6,         # 6 threads ‚úÖ
    f16_kv=True,         # FP16 para KV cache ‚úÖ
    use_mmap=True,       # Memory mapping ‚úÖ
    use_mlock=False      # Sin lock memory ‚úÖ
)
```
**Resultado**: ‚úÖ **EXITOSO** - Modelo Qwen3-Embedding-8B cargado y funcionando
**Tiempo de carga**: ~30 segundos
**Rendimiento**: 65 embeddings en 15.5 segundos

### ‚ùå **CONFIGURACIONES QUE FALLAN**

#### Configuraci√≥n Ultra-Agresiva (FALLA)
```python
n_ctx=16384,         # 16K contexto ‚ùå 
n_gpu_layers=-1,     # Todas las capas ‚ùå
n_batch=2048,        # Batch muy grande ‚ùå
use_mlock=True       # Lock memory ‚ùå
```
**Error**: `Failed to create llama_context`

#### Configuraci√≥n M√°xima (FALLA)
```python
n_ctx=12288,         # 12K contexto ‚ùå
n_gpu_layers=35,     # 35+ capas ‚ùå
```
**Error**: `Failed to create llama_context`

## üìà L√≠mites Exactos Encontrados

### üéØ **Contexto M√°ximo**
- **8192 tokens**: ‚úÖ Funciona perfectamente
- **12288 tokens**: ‚ùå Falla al crear contexto
- **16384 tokens**: ‚ùå Falla al crear contexto

### üéØ **GPU Layers M√°ximo** 
- **30 layers**: ‚úÖ Configuraci√≥n √≥ptima
- **35+ layers**: ‚ùå Excede VRAM disponible

### üéØ **Batch Size √ìptimo**
- **1024**: ‚úÖ Balance perfecto velocidad/memoria
- **2048**: ‚ùå Demasiado para VRAM disponible

### üéØ **Configuraciones de Memoria**
- **f16_kv=True**: ‚úÖ Esencial para ahorrar VRAM
- **use_mmap=True**: ‚úÖ Mejora eficiencia
- **use_mlock=False**: ‚úÖ Evita OOM en sistema
- **offload_kqv=True**: ‚ùå Incompatible con este setup

## üîß Configuraci√≥n de Respaldo (Balanceada)

Si la configuraci√≥n agresiva falla en futuras actualizaciones:

```python
Llama(
    n_ctx=6144,          # 6K contexto (m√°s conservador)
    n_gpu_layers=25,     # 25 capas GPU
    n_batch=512,         # Batch moderado  
    n_threads=4,         # 4 threads
    f16_kv=True,
    use_mmap=True,
    use_mlock=False
)
```

## üéÆ Configuraci√≥n M√≠nima Garantizada

Para compatibilidad m√°xima:

```python
Llama(
    n_ctx=4096,          # 4K contexto (est√°ndar)
    n_gpu_layers=20,     # 20 capas GPU
    n_batch=256,         # Batch peque√±o
    n_threads=2,         # 2 threads
    f16_kv=True,
    use_mmap=True,
    use_mlock=False
)
```

## üìä Rendimiento por Configuraci√≥n

| Configuraci√≥n | Contexto | GPU Layers | Tiempo Carga | Rendimiento | Estado |
|---------------|----------|------------|--------------|-------------|---------|
| Agresiva      | 8192     | 30         | ~30s         | 15.5s/65emb | ‚úÖ √ìptima |
| Balanceada    | 6144     | 25         | ~25s         | ~20s/65emb  | ‚úÖ Backup |
| Conservadora  | 4096     | 20         | ~20s         | ~30s/65emb  | ‚úÖ Segura |
| M√≠nima        | 2048     | 15         | ~15s         | ~45s/65emb  | ‚úÖ Compat |

## ‚ö†Ô∏è Advertencias y Limitaciones

### üö´ **NO Funciona**
- **Modelos simult√°neos**: 2 modelos de 8B al mismo tiempo
- **Contexto >8K**: Cualquier configuraci√≥n con m√°s de 8192 contexto
- **GPU layers >30**: M√°s de 30 capas causa OOM
- **Memory locking**: `use_mlock=True` causa fallos

### ‚ö° **Optimizaciones Clave**
- **FP16 KV Cache**: Fundamental para ahorrar VRAM
- **Memory mapping**: Mejora significativa de rendimiento  
- **Batch moderado**: 1024 es el sweet spot
- **Threading**: 6 threads es √≥ptimo para este sistema

## üîç M√©todo de Detecci√≥n Autom√°tica

El sistema implementa detecci√≥n autom√°tica que prueba configuraciones en este orden:

1. **Agresiva** (8192 ctx, 30 layers) - Probada primero
2. **Balanceada** (6144 ctx, 25 layers) - Si falla agresiva
3. **Conservadora** (4096 ctx, 20 layers) - Si falla balanceada  
4. **M√≠nima** (2048 ctx, 15 layers) - √öltima opci√≥n

## üìù Notas de Implementaci√≥n

- **Cache persistente**: La configuraci√≥n √≥ptima se guarda en `cache/god_class_refactor/optimal_config.json`
- **Reset disponible**: `--reset-config` para forzar nueva detecci√≥n
- **Fallback graceful**: Si falla una configuraci√≥n, prueba la siguiente autom√°ticamente
- **Mensajes informativos**: El sistema reporta qu√© configuraci√≥n est√° usando

---

**Fecha**: Julio 2025  
**Sistema**: 8GB VRAM GPU  
**Modelo**: Qwen3-Embedding-8B-Q6_K.gguf  
**Configuraci√≥n √ìptima**: Agresiva (8192 ctx, 30 layers) ‚úÖ